{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import utils\n",
    "\n",
    "from superpoint.superpoint import SuperPointFrontend\n",
    "from utils.visualization import visual_atten\n",
    "from superpoint.utils import get_query_img_name, get_refer_img_name\n",
    "from utils.visualization import read_failed_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read failed cases from specific file saved by pickle.dump()\n",
    "with open(\"config/eval_config.yaml\", \"r\") as fin:\n",
    "    config = yaml.safe_load(fin)\n",
    "\n",
    "DATASET = config[\"dataset\"][\"name\"]\n",
    "ATTEN_PATH = (\n",
    "    config[\"results_path\"]\n",
    "    + DATASET\n",
    "    + \"/\"\n",
    "    + config[\"output_1\"][\"method\"]\n",
    "    + \"/\"\n",
    "    + config[\"output_1\"][\"anchor_select_policy\"]\n",
    "    + \"/failed_cases\"\n",
    ")\n",
    "FULL_GEO_PATH = (\n",
    "    config[\"results_path\"]\n",
    "    + DATASET\n",
    "    + \"/\"\n",
    "    + config[\"output_2\"][\"method\"]\n",
    "    + \"/\"\n",
    "    + config[\"output_2\"][\"anchor_select_policy\"]\n",
    "    + \"/failed_cases\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the filed cases for attention patch\n",
    "atten_failed_cases = read_failed_cases(ATTEN_PATH)\n",
    "\n",
    "# read the filed cases for attention patch\n",
    "all_failed_cases = read_failed_cases(FULL_GEO_PATH)\n",
    "\n",
    "print(f\"The number of failed cases in atten patch is {len(atten_failed_cases)}\")\n",
    "print(f\"The number of failed cases in cross match is {len(all_failed_cases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PATH = config[\"dataset\"][\"Root\"] + config[\"dataset\"][\"query_dir\"]\n",
    "REF_PATH = config[\"dataset\"][\"Root\"] + config[\"dataset\"][\"refer_dir\"]\n",
    "\n",
    "# Plot the failed cases\n",
    "# display_failed_images(atten_failed_cases, QUERY_PATH, REF_PATH)\n",
    "# display_failed_images(all_failed_cases, QUERY_PATH, REF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the common and differences between all and atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization import DatasetComparer\n",
    "\n",
    "comparer = DatasetComparer(atten_failed_cases, all_failed_cases)\n",
    "comparer.compare_datasets()\n",
    "same_wrong_preds = comparer.are_wrong_predictions_same()\n",
    "\n",
    "same_wrong_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_data_atten = comparer.find_data_for_different_queries()\n",
    "different_data_atten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_descriptors = []\n",
    "refer_descriptors = []\n",
    "wrong_pred_descriptors = []\n",
    "query_anchors = []\n",
    "\n",
    "query_rgbs = []\n",
    "refer_rgbs = []\n",
    "wrong_pred_rgbs = []\n",
    "\n",
    "# Stub to warn about opencv version.\n",
    "if int(cv2.__version__[0]) < 3:  # pragma: no cover\n",
    "    print(\"Warning: OpenCV 3 is not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==> Loading pre-trained network...\")\n",
    "fe = SuperPointFrontend(\n",
    "    weights_path=config[\"model\"][\"weights_path\"],\n",
    "    nms_dist=config[\"model\"][\"nms_dist\"],\n",
    "    conf_thresh=config[\"model\"][\"conf_thresh\"],\n",
    "    nn_thresh=config[\"model\"][\"nn_thresh\"],\n",
    "    cuda=config[\"model\"][\"cuda\"],\n",
    ")\n",
    "\n",
    "print(\"===> Successfully loaded pre-trained network.\")\n",
    "\n",
    "for i in tqdm(range(len(different_data_atten))):\n",
    "    # print('==> Refer: ' + str(refer + refer_index_offset))\n",
    "    try:\n",
    "        refer_img = cv2.imread(\n",
    "            REF_PATH + \"/\" + get_refer_img_name(\"SPED\", different_data_atten[i][2][0])\n",
    "        )\n",
    "        query_ori = cv2.imread(\n",
    "            QUERY_PATH + \"/\" + get_query_img_name(\"SPED\", different_data_atten[i][0])\n",
    "        )\n",
    "        wrong_pred_img = cv2.imread(\n",
    "            REF_PATH + \"/\" + get_query_img_name(\"SPED\", different_data_atten[i][1])\n",
    "        )\n",
    "\n",
    "    except (IOError, ValueError) as e:\n",
    "        refer_img = None\n",
    "        print(\"Exception! \\n \\n \\n \\n\")\n",
    "\n",
    "    if config[\"output_1\"][\"anchor_select_policy\"] == \"conv_filter\":\n",
    "        edges_query = cv2.Canny(query_ori, 300, 1000, apertureSize=5)\n",
    "        # deresolution to 64 * 64\n",
    "        edges_query = cv2.resize(edges_query, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    refer_img = cv2.resize(refer_img, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    refer_img = refer_img.astype(\"float32\") / 255.0\n",
    "    refer_img = cv2.cvtColor(refer_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # for query and wrong prediction\n",
    "    query_img = cv2.resize(query_ori, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    query_img = query_img.astype(\"float32\") / 255.0\n",
    "    query_img = cv2.cvtColor(query_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    wrong_pred_img = cv2.resize(\n",
    "        wrong_pred_img, (256, 256), interpolation=cv2.INTER_AREA\n",
    "    )\n",
    "    wrong_pred_img = wrong_pred_img.astype(\"float32\") / 255.0\n",
    "    wrong_pred_img = cv2.cvtColor(wrong_pred_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    keypoints, desc = fe.run_with_point(refer_img)\n",
    "    refer_descriptors.append(desc)\n",
    "\n",
    "    # anchors\n",
    "    anchors = np.array([], dtype=int)\n",
    "\n",
    "    keypoints = keypoints[:2, :]\n",
    "    keypoints = keypoints.transpose()\n",
    "    keypoints = [[item // 8 for item in subl] for subl in keypoints]\n",
    "    keypoints = [list(t) for t in set(tuple(element) for element in keypoints)]\n",
    "    anchors = np.array(\n",
    "        [utils.idx_table[int(item[0]), int(item[1])] for item in keypoints]\n",
    "    )\n",
    "\n",
    "    query_anchors.append(anchors)\n",
    "\n",
    "    desc = fe.run(query_img)\n",
    "    query_descriptors.append(desc)\n",
    "\n",
    "    desc = fe.run(wrong_pred_img)\n",
    "    wrong_pred_descriptors.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(different_data_atten)):\n",
    "    query_descriptor = query_descriptors[i]\n",
    "    refer_descriptor = refer_descriptors[i]\n",
    "    wrong_pred_descriptor = wrong_pred_descriptors[i]\n",
    "\n",
    "    anchors = query_anchors[i]\n",
    "\n",
    "    query_img_1, refer_q_img, score_1 = visual_atten(\n",
    "        query_descriptor,\n",
    "        refer_descriptor,\n",
    "        anchors,\n",
    "        i,\n",
    "        True,\n",
    "        QUERY_PATH,\n",
    "        REF_PATH,\n",
    "        different_data_atten,\n",
    "    )\n",
    "    query_img_2, wrong_pred, score_2 = visual_atten(\n",
    "        query_descriptor,\n",
    "        wrong_pred_descriptor,\n",
    "        anchors,\n",
    "        i,\n",
    "        False,\n",
    "        QUERY_PATH,\n",
    "        REF_PATH,\n",
    "        different_data_atten,\n",
    "    )\n",
    "\n",
    "    print(f\"Score for query is {score_1}\")\n",
    "    print(f\"Score for wrong prediction is {score_2}\")\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    # Set titles for each subplot\n",
    "    titles = [\"query\", \"reference\", \"query\", \"wrong prediction\"]\n",
    "    # Plot the subimages and set titles\n",
    "\n",
    "    axs[0].imshow(query_img_1)\n",
    "    axs[0].set_title(titles[0])\n",
    "    axs[1].imshow(refer_q_img)\n",
    "    axs[1].set_title(titles[1])\n",
    "    axs[2].imshow(query_img_2)\n",
    "    axs[2].set_title(titles[2])\n",
    "    axs[3].imshow(wrong_pred)\n",
    "    axs[3].set_title(titles[3])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robust-point",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
